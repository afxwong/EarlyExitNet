{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Requirement already satisfied: torch in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from -r requirements.txt (line 1)) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from -r requirements.txt (line 2)) (0.16.0+cu118)\n",
      "Requirement already satisfied: tqdm in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from -r requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from -r requirements.txt (line 4)) (1.24.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from -r requirements.txt (line 5)) (2.0.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from -r requirements.txt (line 6)) (3.7.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from -r requirements.txt (line 7)) (2.14.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from torchvision->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from torchvision->-r requirements.txt (line 2)) (10.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from tqdm->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (6.1.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from datasets->-r requirements.txt (line 7)) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from datasets->-r requirements.txt (line 7)) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from datasets->-r requirements.txt (line 7)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from datasets->-r requirements.txt (line 7)) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from datasets->-r requirements.txt (line 7)) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from datasets->-r requirements.txt (line 7)) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from datasets->-r requirements.txt (line 7)) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 6)) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\austin\\anaconda3\\envs\\env\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0+cu118\n",
      "Is MPS (Metal Performance Shader) built? False\n",
      "Is MPS available? False\n",
      "Is CUDA available? True\n",
      "Using device: device\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Check for CUDA support\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the pretrained vgg code\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists('cifar10_models'):\n",
    "    !git clone https://github.com/huyvnphan/PyTorch_CIFAR10\n",
    "    \n",
    "    # copy cifar10_models folder to current directory\n",
    "    shutil.copytree(os.path.join(\"PyTorch_CIFAR10\", \"cifar10_models\"), \"cifar10_models\")\n",
    "    \n",
    "    # delete the cloned repo\n",
    "    try:\n",
    "        shutil.rmtree(\"PyTorch_CIFAR10\")\n",
    "    except: pass\n",
    "    \n",
    "\n",
    "from cifar10_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick which model to load\n",
    "model_name = \"vgg_cifar100\" # either \"resnet\" or \"vgg_cifar10\" or \"vgg_cifar100\"\n",
    "\n",
    "model_path = os.path.join(\"models\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EarlyExitModel(\n",
       "  (model): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): OptionalExitModule(\n",
       "        (module): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (exit_gate): Linear(in_features=8192, out_features=1, bias=True)\n",
       "        (classifier): Linear(in_features=8192, out_features=100, bias=True)\n",
       "      )\n",
       "      (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU(inplace=True)\n",
       "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (15): OptionalExitModule(\n",
       "        (module): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (exit_gate): Linear(in_features=4096, out_features=1, bias=True)\n",
       "        (classifier): Linear(in_features=4096, out_features=100, bias=True)\n",
       "      )\n",
       "      (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (22): OptionalExitModule(\n",
       "        (module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (exit_gate): Linear(in_features=2048, out_features=1, bias=True)\n",
       "        (classifier): Linear(in_features=2048, out_features=100, bias=True)\n",
       "      )\n",
       "      (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (24): ReLU(inplace=True)\n",
       "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): OptionalExitModule(\n",
       "        (module): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (exit_gate): Linear(in_features=512, out_features=1, bias=True)\n",
       "        (classifier): Linear(in_features=512, out_features=100, bias=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=512, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from EarlyExitModel import EarlyExitModel\n",
    "import pickle\n",
    "import io\n",
    "from torch.hub import *\n",
    "\n",
    "# Need a custom unpickler to load the model onto the CPU since it was trained on a GPU and dumped, not saved\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        print(module, name)\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        elif module == 'pytorch_cifar_models.vgg' and name == \"VGG\":\n",
    "            return None\n",
    "        else: return super().find_class(module, name)\n",
    "\n",
    "def load_model():\n",
    "    # Load the model (classifiers trained, but no gates)\n",
    "    # Load the model onto the CPU\n",
    "    #f = open(os.path.join(model_path, \"final_classifier.pkl\"), \"rb\")\n",
    "    #model = CPU_Unpickler(f).load()\n",
    "\n",
    "    # Load the pickled model from the file\n",
    "    model = pickle.load(open(os.path.join(model_path, \"final_classifier.pkl\"), 'rb'))\n",
    "\n",
    "    # Set the device on the model and its submodules\n",
    "    model.device = device    \n",
    "    model = model.to(device)\n",
    "    model.costs = model.compute_costs_per_exit_module()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from DataLoader import CustomDataset\n",
    "import numpy as np\n",
    "\n",
    "if model_name == \"resnet\":\n",
    "    # use the imagenette dataset\n",
    "    hf_dataset = load_dataset(\"frgfm/imagenette\", '320px')\n",
    "    hf_dataset = concatenate_datasets(hf_dataset.values())\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "elif model_name == \"vgg_cifar10\":\n",
    "    # use the cifar10 dataset\n",
    "    hf_dataset = load_dataset(\"cifar10\")\n",
    "    hf_dataset = concatenate_datasets(hf_dataset.values())\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.507, 0.4865, 0.4409],\n",
    "                             std=[0.2673, 0.2564, 0.2761])\n",
    "    ])\n",
    "elif model_name == \"vgg_cifar100\":\n",
    "    # use the cifar100 dataset\n",
    "    hf_dataset = load_dataset(\"cifar100\")\n",
    "    hf_dataset = concatenate_datasets(hf_dataset.values())\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "torch_dataset = CustomDataset(hf_dataset, transform=transform)\n",
    "\n",
    "batch_size = 32 if model_name == \"resnet\" else 64\n",
    "\n",
    "test_size = 0.2\n",
    "test_volume = int(test_size * len(torch_dataset))\n",
    "train_volume = len(torch_dataset) - test_volume\n",
    "\n",
    "train_dataset, test_dataset = random_split(torch_dataset, [train_volume, test_volume])\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Exit Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want tensorboard support, you need to run the following commands on **macOS**:\n",
    "\n",
    "This clears your logs:\n",
    "```\n",
    "rm -rf runs/*\n",
    "```\n",
    "\n",
    "This shows your public IP address:\n",
    "```\n",
    "dig -4 TXT +short o-o.myaddr.l.google.com @ns1.google.com\n",
    "```\n",
    "\n",
    "This starts tensorboard at this address:\n",
    "```\n",
    "tensorboard --host 0.0.0.0 --logdir={model_path}/runs\n",
    "```\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want tensorboard support, you need to run the following commands on **Windows**:\n",
    "\n",
    "This clears your logs:\n",
    "```\n",
    "rmdir /s /q runs\n",
    "```\n",
    "\n",
    "This shows your IP address:\n",
    "```\n",
    "powershell -command \"$ipAddress = (Invoke-WebRequest -Uri 'http://ipinfo.io/ip').Content.Trim(); Write-Host 'Your IP address is: ' $ipAddress\"  \n",
    "```\n",
    "\n",
    "This starts tensorboard at this address:\n",
    "```\n",
    "tensorboard --host 0.0.0.0 --logdir={model_path}\\runs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EarlyExitTrainer import ModelTrainer\n",
    "\n",
    "trainer = ModelTrainer(model, device, model_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with alpha=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000001EEDEC33700>       \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1436, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Austin\\Downloads\\EarlyExitNet\\alpha_tuning.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Downloads/EarlyExitNet/alpha_tuning.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m trainer\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m load_model() \u001b[39m# reset the model each time\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Downloads/EarlyExitNet/alpha_tuning.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m trainer\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Austin/Downloads/EarlyExitNet/alpha_tuning.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m final_accuracy, final_time, final_avg_exit_idx \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain_exit_layers(train_dataloader, lr, epoch_count\u001b[39m=\u001b[39;49mepoch_count, validation_loader\u001b[39m=\u001b[39;49mtest_dataloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Downloads/EarlyExitNet/alpha_tuning.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# write statistics\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Downloads/EarlyExitNet/alpha_tuning.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m accuracies[alpha] \u001b[39m=\u001b[39m final_accuracy\n",
      "File \u001b[1;32mc:\\Users\\Austin\\Downloads\\EarlyExitNet\\EarlyExitTrainer.py:206\u001b[0m, in \u001b[0;36mModelTrainer.train_exit_layers\u001b[1;34m(self, train_loader, lr, epoch_count, validation_loader)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_count):\n\u001b[0;32m    202\u001b[0m     \u001b[39m# run the train cycle\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \n\u001b[0;32m    204\u001b[0m     \u001b[39m# set model state\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mset_state(TrainingState\u001b[39m.\u001b[39mTRAIN_EXIT)\n\u001b[1;32m--> 206\u001b[0m     loss, validation_accuracy, validation_time, exit_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_exit_epoch(train_loader, lr, epoch, validation_loader)\n\u001b[0;32m    207\u001b[0m     validation_accuracies\u001b[39m.\u001b[39mappend(validation_accuracy)\n\u001b[0;32m    208\u001b[0m     validation_times\u001b[39m.\u001b[39mappend(validation_time)\n",
      "File \u001b[1;32mc:\\Users\\Austin\\Downloads\\EarlyExitNet\\EarlyExitTrainer.py:178\u001b[0m, in \u001b[0;36mModelTrainer.train_exit_epoch\u001b[1;34m(self, train_loader, lr, epoch, validation_loader)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39m# Optionally, calculate validation metrics\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39mif\u001b[39;00m validation_loader \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m i \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(train_loader) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m10\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 178\u001b[0m     validation_accuracy, validation_time, exit_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_exit_gates(validation_loader)\n\u001b[0;32m    180\u001b[0m     \u001b[39m# write to tensorboard\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39madd_scalar(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss/alpha=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha\u001b[39m}\u001b[39;00m\u001b[39m/exit gates\u001b[39m\u001b[39m'\u001b[39m, loss, (epoch \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(train_loader) \u001b[39m+\u001b[39m i))\n",
      "File \u001b[1;32mc:\\Users\\Austin\\Downloads\\EarlyExitNet\\EarlyExitTrainer.py:266\u001b[0m, in \u001b[0;36mModelTrainer.validate_exit_gates\u001b[1;34m(self, validation_loader)\u001b[0m\n\u001b[0;32m    264\u001b[0m X_val \u001b[39m=\u001b[39m X_val\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    265\u001b[0m y_val \u001b[39m=\u001b[39m y_val\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 266\u001b[0m y_hat_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(X_val)\n\u001b[0;32m    268\u001b[0m totaltime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m starttime\n\u001b[0;32m    269\u001b[0m val_accuracy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_accuracy(y_hat_val, y_val)\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Austin\\Downloads\\EarlyExitNet\\EarlyExitModel.py:105\u001b[0m, in \u001b[0;36mEarlyExitModel.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    103\u001b[0m final_exit_y_hat \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     last_layer_y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(X)\n\u001b[0;32m    106\u001b[0m \u001b[39mexcept\u001b[39;00m EarlyExitException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    107\u001b[0m     final_exit_y_hat \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39my_hat\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\\pytorch_cifar_models\\vgg.py:83\u001b[0m, in \u001b[0;36mVGG.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> 83\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m     84\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n\u001b[0;32m     85\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(x)\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Austin\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2479\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2480\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the exits\n",
    "alpha_range = [i / 100 for i in range(0, 101, 10)] #[0, 0.25, 0.5, 0.55, 0.6, 0.65, 0.7, 0.725, 0.75, 0.775] + [i / 100 for i in range(80, 101, 2)]\n",
    "\n",
    "accuracies = {}\n",
    "times = {}\n",
    "avg_exit_idx = {}\n",
    "\n",
    "\n",
    "epoch_count = 3 if model_name == \"resnet\" else 3\n",
    "lr = 0.00000001 if model_name == \"resnet\" else 0.000001\n",
    "\n",
    "for alpha in alpha_range:\n",
    "    print(f\"Training with alpha={alpha}\")\n",
    "    trainer.model = load_model() # reset the model each time\n",
    "    trainer.set_alpha(alpha)\n",
    "    final_accuracy, final_time, final_avg_exit_idx = trainer.train_exit_layers(train_dataloader, lr, epoch_count=epoch_count, validation_loader=test_dataloader)\n",
    "    \n",
    "    # write statistics\n",
    "    accuracies[alpha] = final_accuracy\n",
    "    times[alpha] = final_time\n",
    "    avg_exit_idx[alpha] = final_avg_exit_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "      <th>avg_exit_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.976895</td>\n",
       "      <td>0.011893</td>\n",
       "      <td>3.918301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.975565</td>\n",
       "      <td>0.008021</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.975565</td>\n",
       "      <td>0.007994</td>\n",
       "      <td>2.999751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.968085</td>\n",
       "      <td>0.008366</td>\n",
       "      <td>2.914894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.870013</td>\n",
       "      <td>0.006532</td>\n",
       "      <td>2.039146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.868351</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>2.047124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>0.743434</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>1.031416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>0.743434</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.743434</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>1.031416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>0.743434</td>\n",
       "      <td>0.004117</td>\n",
       "      <td>1.031416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.743434</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>1.031416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy      time  avg_exit_idx\n",
       "0.0  0.976895  0.011893      3.918301\n",
       "0.1  0.975565  0.008021      3.000000\n",
       "0.2  0.975565  0.007994      2.999751\n",
       "0.3  0.968085  0.008366      2.914894\n",
       "0.4  0.870013  0.006532      2.039146\n",
       "0.5  0.868351  0.005585      2.047124\n",
       "0.6  0.743434  0.003797      1.031416\n",
       "0.7  0.743434  0.004303      1.000000\n",
       "0.8  0.743434  0.003819      1.031416\n",
       "0.9  0.743434  0.004117      1.031416\n",
       "1.0  0.743434  0.003968      1.031416"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "# generate dataframe containing statistics\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"accuracy\": accuracies,\n",
    "    \"time\": times,\n",
    "    \"avg_exit_idx\": avg_exit_idx\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "\n",
    "df.to_csv(os.path.join(model_path, \"alpha_tuning.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
