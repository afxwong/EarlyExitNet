{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0\n",
      "Is MPS (Metal Performance Shader) built? True\n",
      "Is MPS available? True\n",
      "Is CUDA available? False\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Check for CUDA support\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick which model to load\n",
    "model_name = \"vgg_cifar100\" # either \"resnet\" or \"vgg_cifar10\" or \"vgg_cifar100\"\n",
    "num_classes = 100 if model_name == \"vgg_cifar100\" else 10\n",
    "model_path = os.path.join(\"models\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from DataLoader import CustomDataset\n",
    "\n",
    "if model_name == \"resnet\":\n",
    "    # use the imagenette dataset\n",
    "    hf_dataset = load_dataset(\"frgfm/imagenette\", '320px')\n",
    "    hf_dataset = concatenate_datasets(hf_dataset.values())\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "elif model_name == \"vgg_cifar10\":\n",
    "    # use the cifar10 dataset\n",
    "    hf_dataset = load_dataset(\"cifar10\")\n",
    "    hf_dataset = concatenate_datasets(hf_dataset.values())\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.507, 0.4865, 0.4409],\n",
    "                             std=[0.2673, 0.2564, 0.2761])\n",
    "    ])\n",
    "elif model_name == \"vgg_cifar100\":\n",
    "    # use the cifar100 dataset\n",
    "    hf_dataset = load_dataset(\"cifar100\")\n",
    "    hf_dataset = concatenate_datasets(hf_dataset.values())\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "torch_dataset = CustomDataset(hf_dataset, transform=transform)\n",
    "\n",
    "batch_size = 32 if model_name == \"resnet\" else 64\n",
    "\n",
    "test_size = 0.2\n",
    "test_volume = int(test_size * len(torch_dataset))\n",
    "train_volume = len(torch_dataset) - test_volume\n",
    "\n",
    "train_dataset, test_dataset = random_split(torch_dataset, [train_volume, test_volume])\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EarlyExit VGG11 model architecture...\n",
      "Adding exits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/dylanmace/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting model weights...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EarlyExitModel(\n",
       "  (model): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): OptionalExitModule(\n",
       "        (module): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (exit_gate): Linear(in_features=8192, out_features=1, bias=True)\n",
       "        (classifier): Linear(in_features=8192, out_features=100, bias=True)\n",
       "      )\n",
       "      (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU(inplace=True)\n",
       "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (15): OptionalExitModule(\n",
       "        (module): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (exit_gate): Linear(in_features=4096, out_features=1, bias=True)\n",
       "        (classifier): Linear(in_features=4096, out_features=100, bias=True)\n",
       "      )\n",
       "      (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (22): OptionalExitModule(\n",
       "        (module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (exit_gate): Linear(in_features=2048, out_features=1, bias=True)\n",
       "        (classifier): Linear(in_features=2048, out_features=100, bias=True)\n",
       "      )\n",
       "      (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (24): ReLU(inplace=True)\n",
       "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): OptionalExitModule(\n",
       "        (module): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (exit_gate): Linear(in_features=512, out_features=1, bias=True)\n",
       "        (classifier): Linear(in_features=512, out_features=100, bias=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=512, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ModelLoader\n",
    "\n",
    "loader = ModelLoader.ModelLoader(model_name, device, alpha=0.6, dataloader=train_dataloader)\n",
    "\n",
    "# preview the model architecture\n",
    "model = loader.load_model(num_outputs=num_classes, pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Exit Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "  \n",
    "if model_name == \"vgg_cifar100\":\n",
    "    class_names = [\n",
    "        \"apple\",\n",
    "        \"aquarium_fish\",\n",
    "        \"baby\",\n",
    "        \"bear\",\n",
    "        \"beaver\",\n",
    "        \"bed\",\n",
    "        \"bee\",\n",
    "        \"beetle\",\n",
    "        \"bicycle\",\n",
    "        \"bottle\",\n",
    "        \"bowl\",\n",
    "        \"boy\",\n",
    "        \"bridge\",\n",
    "        \"bus\",\n",
    "        \"butterfly\",\n",
    "        \"camel\",\n",
    "        \"can\",\n",
    "        \"castle\",\n",
    "        \"caterpillar\",\n",
    "        \"cattle\",\n",
    "        \"chair\",\n",
    "        \"chimpanzee\",\n",
    "        \"clock\",\n",
    "        \"cloud\",\n",
    "        \"cockroach\",\n",
    "        \"couch\",\n",
    "        \"crab\",\n",
    "        \"crocodile\",\n",
    "        \"cup\",\n",
    "        \"dinosaur\",\n",
    "        \"dolphin\",\n",
    "        \"elephant\",\n",
    "        \"flatfish\",\n",
    "        \"forest\",\n",
    "        \"fox\",\n",
    "        \"girl\",\n",
    "        \"hamster\",\n",
    "        \"house\",\n",
    "        \"kangaroo\",\n",
    "        \"keyboard\",\n",
    "        \"lamp\",\n",
    "        \"lawn_mower\",\n",
    "        \"leopard\",\n",
    "        \"lion\",\n",
    "        \"lizard\",\n",
    "        \"lobster\",\n",
    "        \"man\",\n",
    "        \"maple_tree\",\n",
    "        \"motorcycle\",\n",
    "        \"mountain\",\n",
    "        \"mouse\",\n",
    "        \"mushroom\",\n",
    "        \"oak_tree\",\n",
    "        \"orange\",\n",
    "        \"orchid\",\n",
    "        \"otter\",\n",
    "        \"palm_tree\",\n",
    "        \"pear\",\n",
    "        \"pickup_truck\",\n",
    "        \"pine_tree\",\n",
    "        \"plain\",\n",
    "        \"plate\",\n",
    "        \"poppy\",\n",
    "        \"porcupine\",\n",
    "        \"possum\",\n",
    "        \"rabbit\",\n",
    "        \"raccoon\",\n",
    "        \"ray\",\n",
    "        \"road\",\n",
    "        \"rocket\",\n",
    "        \"rose\",\n",
    "        \"sea\",\n",
    "        \"seal\",\n",
    "        \"shark\",\n",
    "        \"shrew\",\n",
    "        \"skunk\",\n",
    "        \"skyscraper\",\n",
    "        \"snail\",\n",
    "        \"snake\",\n",
    "        \"spider\",\n",
    "        \"squirrel\",\n",
    "        \"streetcar\",\n",
    "        \"sunflower\",\n",
    "        \"sweet_pepper\",\n",
    "        \"table\",\n",
    "        \"tank\",\n",
    "        \"telephone\",\n",
    "        \"television\",\n",
    "        \"tiger\",\n",
    "        \"tractor\",\n",
    "        \"train\",\n",
    "        \"trout\",\n",
    "        \"tulip\",\n",
    "        \"turtle\",\n",
    "        \"wardrobe\",\n",
    "        \"whale\",\n",
    "        \"willow_tree\",\n",
    "        \"wolf\",\n",
    "        \"woman\",\n",
    "        \"worm\"\n",
    "    ]\n",
    "else:\n",
    "    class_names = []\n",
    "    \n",
    "if model_name == \"vgg_cifar10\":\n",
    "    class_idx = 5\n",
    "elif model_name == \"vgg_cifar100\":\n",
    "    class_idx = class_names.index(\"wolf\")\n",
    "else:\n",
    "    class_idx = 0\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the images of the class_idx\n",
    "class_images = []\n",
    "for i in range(len(torch_dataset)):\n",
    "    img, label = torch_dataset[i]\n",
    "    if label == class_idx:\n",
    "        class_images.append((img, label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images of class wolf\n",
      "Shape of first image: torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(class_images)} images of class {class_names[class_idx]}\")\n",
    "print(f\"Shape of first image: {class_images[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images: {3: 1}\n",
      "Misclassified images: 1\n",
      "Correct images: 0\n",
      "Processed 100 images: {3: 28, 2: 73}\n",
      "Misclassified images: 72\n",
      "Correct images: 29\n",
      "Processed 200 images: {3: 55, 2: 146}\n",
      "Misclassified images: 142\n",
      "Correct images: 59\n",
      "Processed 300 images: {3: 88, 2: 213}\n",
      "Misclassified images: 215\n",
      "Correct images: 86\n",
      "Processed 400 images: {3: 117, 2: 284}\n",
      "Misclassified images: 291\n",
      "Correct images: 110\n",
      "Processed 500 images: {3: 145, 2: 356}\n",
      "Misclassified images: 371\n",
      "Correct images: 130\n"
     ]
    }
   ],
   "source": [
    "# load each image throughout the model\n",
    "num_exits_taken = {}\n",
    "misclassified_images = {}\n",
    "correct_images = {}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(class_images)):\n",
    "    image, label = class_images[i]\n",
    "    image = image.reshape(1, *image.shape).to(device)\n",
    "    \n",
    "    output_class = model(image)\n",
    "    exit_idx_taken = torch.tensor(model.num_exits_per_module, device=device).argmax() + 1\n",
    "    num_exits_taken[exit_idx_taken.item()] = num_exits_taken.get(exit_idx_taken.item(), 0) + 1\n",
    "    \n",
    "    # get class name for prediction from model\n",
    "    class_name = class_names[output_class.argmax().item()]\n",
    "    \n",
    "    if output_class.argmax() != class_idx:\n",
    "        misclassified_images[i] = (output_class.argmax().item(), label, exit_idx_taken.item())\n",
    "    else:\n",
    "        correct_images[i] = exit_idx_taken.item()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} images:\", num_exits_taken)\n",
    "        print(f\"Misclassified images: {len(misclassified_images)}\")\n",
    "        print(f\"Correct images: {len(correct_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data_vis\"):\n",
    "    os.mkdir(\"data_vis\")\n",
    "    \n",
    "if not os.path.exists(os.path.join(\"data_vis\", \"misclassified\")):\n",
    "    os.mkdir(os.path.join(\"data_vis\", \"misclassified\"))\n",
    "    \n",
    "if not os.path.exists(os.path.join(\"data_vis\", \"correct\")):\n",
    "    os.mkdir(os.path.join(\"data_vis\", \"correct\"))\n",
    "    \n",
    "# clear the folders\n",
    "for filename in os.listdir(os.path.join(\"data_vis\", \"misclassified\")):\n",
    "    os.remove(os.path.join(\"data_vis\", \"misclassified\", filename))\n",
    "    \n",
    "for filename in os.listdir(os.path.join(\"data_vis\", \"correct\")):\n",
    "    os.remove(os.path.join(\"data_vis\", \"correct\", filename))\n",
    "    \n",
    "# save the misclassified images\n",
    "for i, (yhat, y, exit_idx) in misclassified_images.items():\n",
    "    image = class_images[i][0]\n",
    "    misclass_label = class_names[yhat]\n",
    "    torchvision.utils.save_image(image, os.path.join(\"data_vis\", \"misclassified\", f\"{i}-{misclass_label}-idx{exit_idx}.png\"))\n",
    "    \n",
    "# save the correct images\n",
    "for (image_idx, exit_idx) in correct_images.items():\n",
    "    image = class_images[image_idx][0]\n",
    "    torchvision.utils.save_image(image, os.path.join(\"data_vis\", \"correct\", f\"{image_idx}-idx{exit_idx}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 31365, 2: 15860, 3: 0, 4: 775, 5: 0}\n"
     ]
    }
   ],
   "source": [
    "# iterate through the entire dataloader and save the exit idxs\n",
    "exit_idx_counts = {}\n",
    "\n",
    "for i, (x, y) in enumerate(train_dataloader):\n",
    "    y_hat = model(x.to(device))\n",
    "    y_hat_classes = y_hat.argmax(dim=1)\n",
    "    exit_idxs = torch.tensor(model.num_exits_per_module, device=device)\n",
    "    # concat the exit idxs with batch size - sum of exit idxs\n",
    "    exit_idxs = torch.cat((exit_idxs, torch.tensor([batch_size]).to(device) - exit_idxs.sum()))\n",
    "    for (i, exit_idx) in enumerate(exit_idxs):\n",
    "        exit_idx_counts[i] = exit_idx_counts.get(i, 0) + exit_idx.item()\n",
    "print(exit_idx_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
